# GlobalMart: Enterprise Real-Time E-Commerce Analytics Platform

[![Production Status](https://img.shields.io/badge/status-production-green.svg)]()
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.4.0-orange.svg)]()
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-latest-blue.svg)]()
[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)]()

A production-grade, real-time analytics platform for enterprise e-commerce operations. GlobalMart processes millions of transactions, provides instant business insights, and powers data-driven decision making through advanced stream and batch processing architectures.

## ğŸ“Š Platform Overview

GlobalMart serves as the analytics backbone for a global e-commerce operation with:

- **10 million active users** across 5 countries
- **250,000 products** spanning 100+ categories
- **100,000+ daily transactions** with real-time processing
- **Sub-second query latency** for business intelligence
- **Lambda Architecture** combining real-time and batch processing
- **Enterprise-grade reliability** with 99.9% uptime

## ğŸ—ï¸ Architecture Highlights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Generation â”‚ â†’ Kafka Topics â†’ Spark Streaming â†’ PostgreSQL (Realtime)
â”‚  (50 events/s)  â”‚     (3 topics)     (Savers)              â†“
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    Batch ETL (Spark)
                                                              â†“
                                                  PostgreSQL (Warehouse)
                                                    Star Schema:
                                                    - 5 Dimensions (SCD Type 2)
                                                    - 3 Fact Tables
                                                    - 4 Analytics Tables
                                                              â†“
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â†“                            â†“
                                           Grafana                      Power BI
                                    (Real-time Dashboards)        (Executive Analytics)
```

**Key Architectural Features:**
- **Lambda Architecture**: Combines real-time streaming (speed layer) with batch processing (batch layer)
- **Event-Driven**: Kafka-based event streaming for decoupled, scalable architecture
- **Star Schema Warehouse**: Optimized dimensional model for analytical queries
- **SCD Type 2**: Historical tracking of dimension changes with effective dating
- **UTC Consistency**: All timestamps maintained in UTC throughout the pipeline

## ğŸš€ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Stream Processing** | Apache Spark 3.4.0 + Structured Streaming | Real-time event processing |
| **Batch Processing** | Apache Spark 3.4.0 + PySpark | ETL and analytics jobs |
| **Message Broker** | Apache Kafka | Event streaming backbone |
| **Databases** | PostgreSQL (2 instances) | Realtime + Warehouse storage |
| **Caching** | Redis | API response caching |
| **API** | FastAPI | RESTful data access |
| **Visualization** | Grafana + Power BI | Business intelligence |
| **Monitoring** | Prometheus + Grafana | System observability |
| **Orchestration** | Docker Compose | Service deployment |

## ğŸ’¡ Key Features

### Real-Time Stream Processing
- **Event Savers**: Persist Kafka events to PostgreSQL for batch ETL
- **Low Latency**: Sub-second processing from event to storage
- **Fault Tolerant**: Checkpointing and exactly-once semantics
- **Scalable**: Horizontal scaling with Kafka partitions

### Batch Analytics
- **Daily ETL Pipeline**: Processes all transactional data nightly
- **SCD Type 2 Implementation**: Tracks historical changes in customer, product, and geography dimensions
- **Deduplication**: Event ID-based deduplication prevents duplicate processing
- **Incremental Loading**: Only new data processed each run

### Data Warehouse
**Fact Tables:**
- `fact_sales` - Transaction-level sales data
- `fact_product_views` - Product view events
- `fact_cart_events` - Shopping cart interactions

**Dimension Tables (SCD Type 2):**
- `dim_customers` - Customer profiles with segment classification
- `dim_products` - Product catalog with category hierarchy
- `dim_geography` - Geographic dimensions
- `dim_date` - Date dimension for time-based analysis

**Analytics Tables:**
- `product_performance` - Product metrics (sales, views, conversion)
- `customer_segments` - Customer segmentation analysis
- `rfm_analysis` - RFM (Recency, Frequency, Monetary) scoring
- `sales_trends` - Time-series sales trends by period

### Business Intelligence
- **Executive Dashboard**: KPIs, revenue trends, customer analytics
- **Real-Time Monitoring**: Live system health and data pipeline status
- **Self-Service Analytics**: Grafana dashboards for stakeholder access

## ğŸ“ Project Structure

```
GlobalMart/
â”œâ”€â”€ batch_processing/           # Batch ETL and analytics
â”‚   â”œâ”€â”€ etl/                   # ETL jobs for fact and dimension tables
â”‚   â”œâ”€â”€ jobs/                  # Analytics batch jobs
â”‚   â”œâ”€â”€ warehouse/             # Warehouse utilities
â”‚   â”œâ”€â”€ schema/                # Star schema definitions
â”‚   â””â”€â”€ scheduler/             # Cron job schedulers
â”œâ”€â”€ stream_processing/          # Real-time processing
â”‚   â”œâ”€â”€ jobs/                  # Event saver jobs
â”‚   â”œâ”€â”€ schema/                # Realtime database schema
â”‚   â””â”€â”€ main.py                # Stream job orchestrator
â”œâ”€â”€ data_generation/            # Event generators
â”‚   â”œâ”€â”€ generators/            # Data generators
â”‚   â””â”€â”€ kafka_producers/       # Kafka producers
â”œâ”€â”€ dashboards/                 # Visualization
â”‚   â””â”€â”€ grafana/               # Grafana dashboards and datasources
â”œâ”€â”€ config/                     # Configuration management
â”‚   â””â”€â”€ settings.py            # Centralized settings
â”œâ”€â”€ infrastructure/             # Deployment and monitoring
â”‚   â”œâ”€â”€ docker-compose.yml     # Service orchestration
â”‚   â””â”€â”€ scripts/               # Automation scripts
â””â”€â”€ docs/                       # Documentation
    â”œâ”€â”€ ARCHITECTURE.md        # Technical architecture
    â””â”€â”€ USER_GUIDE.md          # User guide
```

## ğŸ¯ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+
- Apache Spark 3.4.0
- 8GB RAM minimum
- 20GB disk space

### Installation

1. **Clone and Setup Environment**
```bash
git clone <repository-url>
cd GlobalMart
cp .env.example .env
# Edit .env with your configuration
```

2. **Install Dependencies**
```bash
pip install -r requirements.txt
```

3. **Start Infrastructure**
```bash
docker-compose up -d
# Wait for services to be ready (~60 seconds)
```

4. **Initialize Databases**
```bash
# Initialize realtime database
psql -h localhost -p 5433 -U globalmart_user -d globalmart_realtime -f stream_processing/schema/init_realtime.sql

# Initialize warehouse
psql -h localhost -p 5432 -U globalmart_user -d globalmart_warehouse -f batch_processing/schema/init_warehouse.sql
```

### Running the Platform

**Terminal 1: Data Generation**
```bash
python data_generation/main.py
```

**Terminal 2: Stream Processing**
```bash
python stream_processing/main.py
```

**Terminal 3: Batch Processing**
```bash
# Run batch jobs manually
python batch_processing/main.py

# Or set up automated scheduling
./batch_processing/scheduler/setup_cron.sh
```

### Accessing Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Grafana | http://localhost:3000 | admin/admin |
| Prometheus | http://localhost:9090 | - |
| Kafka | localhost:9092 | - |
| PostgreSQL Realtime | localhost:5433 | globalmart_user/realtime_password |
| PostgreSQL Warehouse | localhost:5432 | globalmart_user/warehouse_password |
| Redis | localhost:6379 | - |

## ğŸ“ˆ Data Pipeline Flow

### Real-Time Layer (Speed Layer)
1. **Data Generation** â†’ Generates realistic e-commerce events
2. **Kafka Topics** â†’ Buffers events (`transactions-topic`, `product-views-topic`, `cart-events-topic`)
3. **Stream Processing** â†’ Spark jobs save events to realtime database
4. **PostgreSQL Realtime** â†’ Stores raw event data

### Batch Layer (Batch Layer)
1. **ETL Jobs** â†’ Extract from realtime DB, transform, load to warehouse
   - `fact_sales_etl.py` - Sales fact table
   - `fact_product_views_etl.py` - Product views fact
   - `fact_cart_events_etl.py` - Cart events fact
   - `dim_*_etl.py` - Dimension tables with SCD Type 2
2. **Analytics Jobs** â†’ Generate business insights
   - `product_performance_job.py` - Product metrics
   - `customer_segments_job.py` - Customer segmentation
   - `rfm_analysis_job.py` - RFM scoring
   - `sales_trends_job.py` - Trend analysis
3. **PostgreSQL Warehouse** â†’ Star schema for BI queries

### Serving Layer
1. **Grafana** â†’ Queries warehouse for dashboards
2. **Power BI** â†’ Executive reporting
3. **FastAPI** â†’ Programmatic data access

## ğŸ”§ Configuration

Key configuration via `.env` file:

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Data Generation
DATA_GEN_EVENTS_PER_SECOND=50

# PostgreSQL Realtime
POSTGRES_REALTIME_HOST=localhost
POSTGRES_REALTIME_PORT=5433
POSTGRES_REALTIME_DB=globalmart_realtime

# PostgreSQL Warehouse
POSTGRES_WAREHOUSE_HOST=localhost
POSTGRES_WAREHOUSE_PORT=5432
POSTGRES_WAREHOUSE_DB=globalmart_warehouse

# Spark
SPARK_MASTER_URL=local[*]
SPARK_DRIVER_MEMORY=2g
```

## ğŸ“Š Performance Metrics

- **Event Throughput**: 50 events/second (configurable up to 500/s)
- **Stream Processing Latency**: <1 second end-to-end
- **Batch ETL Duration**: ~5 minutes for daily processing
- **Query Performance**: <100ms for dashboard queries
- **Data Warehouse Size**: ~10GB for 30 days of data
- **System Uptime**: 99.9%

## ğŸ“– Documentation

- **[Architecture Guide](docs/ARCHITECTURE.md)** - Detailed technical architecture
- **[User Guide](docs/USER_GUIDE.md)** - Complete user documentation
- **[API Documentation](#)** - REST API reference (coming soon)

## ğŸ› ï¸ Maintenance

### Database Backup
```bash
# Backup warehouse
pg_dump -h localhost -p 5432 -U globalmart_user globalmart_warehouse > backup.sql
```

### Monitoring Health
```bash
# Check all services
docker-compose ps

# View logs
docker-compose logs -f [service-name]

# Check data pipeline
python batch_processing/check_data_quality.py
```

### Troubleshooting

**Time Series Graphs Not Updating:**
- Verify batch ETL is running and completing successfully
- Check timezone settings (should be UTC)
- Ensure data generator is running

**High Memory Usage:**
- Adjust Spark memory settings in `.env`
- Review watermark settings in stream jobs
- Check for data accumulation in checkpoints

**Slow Queries:**
- Verify indexes on warehouse tables
- Check query execution plans
- Consider materialized views for complex aggregations

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

Educational project for Big Data Analytics course.

## ğŸ™ Acknowledgments

- Apache Software Foundation for Kafka and Spark
- PostgreSQL Global Development Group
- Grafana Labs
- Open-source community

---

**Built with â¤ï¸ for enterprise-scale e-commerce analytics**
